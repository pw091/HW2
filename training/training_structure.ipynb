{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLaMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '21.jsonl'\n",
    "TEST_PATH = 'test.jsonl'\n",
    "VAL_PATH = 'val.jsonl'\n",
    "MODEL_PATH = '' #directory w/ model_train.py, and tokenizer_zeropad.py (modified from Meta's codebase)\n",
    "TRAINED_SPM_PATH = '' #directory w/ tokenizer.model (trained sentencepiece tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(MODEL_PATH) #where transformer and tokenizer are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "from model_train import ModelArgs, Transformer #ctrl+f and comment out cuda (no GPU), removed caching, configured Transformer.forward for training\n",
    "from tokenizer_zeropad import Tokenizer #custom padding value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_list(filepath:str, maxiter:int) -> List[dict]:\n",
    "    '''ingests JSON into list (with tripwire parameter to prevent computer from crashing)'''\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= maxiter:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in texts:\n",
    "            encodings = tokenizer.encode(text, bos=True, eos=True) #beginning/end of sentence tokens\n",
    "\n",
    "            #takes all but the last token as input and all but the first token as target\n",
    "            self.inputs.append(torch.tensor(encodings[:-1], dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(encodings[1:], dtype=torch.long))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.inputs[idx],\n",
    "                \"target_ids\": self.targets[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''custom collation function for tokenized sequences'''\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    \n",
    "    max_seq_len = 2048 #fixed tensor dimension for sequence lengths\n",
    "    input_ids = [ids[:max_seq_len] for ids in input_ids]\n",
    "    target_ids = [ids[:max_seq_len] for ids in target_ids]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0) #add padding\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "    return {'input_ids': input_ids, 'target_ids': target_ids}\n",
    "\n",
    "train_data = make_data_list(TRAIN_PATH, 10)\n",
    "test_data = make_data_list(TEST_PATH, 2)\n",
    "val_data = make_data_list(VAL_PATH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(data_list):\n",
    "    '''gets rid of the metadata'''\n",
    "    return [item['text'] for item in data_list]\n",
    "\n",
    "train_texts = extract_texts(train_data)\n",
    "test_texts = extract_texts(test_data)\n",
    "val_texts = extract_texts(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(TRAINED_SPM_PATH)\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment for CPU (suboptimal for GPU; no parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "\n",
    "%env RANK=0\n",
    "%env WORLD_SIZE=1\n",
    "%env MASTER_ADDR=localhost\n",
    "%env MASTER_PORT=0\n",
    "\n",
    "torch.distributed.init_process_group(backend='gloo')\n",
    "fs_init.initialize_model_parallel(1) #1 worker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarily chosen for prototyping\n",
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    vocab_size=tokenizer.n_words,\n",
    "    multiple_of=256,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=32,\n",
    "    max_seq_len=2048,\n",
    ")\n",
    "\n",
    "model = Transformer(model_args)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)  #ignores padding token (0) for loss calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_tensor(tensor):\n",
    "    '''debugging function'''\n",
    "    print(tensor)\n",
    "    print(\"Type:\", tensor.type())\n",
    "    print(\"Data Type:\", tensor.dtype)\n",
    "    print(\"Shape:\", tensor.shape)\n",
    "    print(\"Size:\", tensor.size())\n",
    "    print(\"Number of Dimensions:\", tensor.ndim)\n",
    "    print(\"Device:\", tensor.device)\n",
    "    print(\"Requires Grad:\", tensor.requires_grad)\n",
    "    print(\"Gradient:\", tensor.grad)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_function, num_epochs):\n",
    "    '''the training loop'''\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()  #reset gradients\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "            target_ids = batch['target_ids']\n",
    "            outputs = model(input_ids, start_pos=0) #forward pass\n",
    "            \n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), target_ids.view(-1))\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch, total_loss / len(dataloader)))\n",
    "        \n",
    "num_epochs = 1\n",
    "train(model, train_dataloader, optimizer, loss_function, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMATH-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
